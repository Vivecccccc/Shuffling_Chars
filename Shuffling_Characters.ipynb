{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:43.271974Z",
     "start_time": "2020-12-18T09:48:43.266988Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import seqeval.metrics\n",
    "from transformers import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torchcrf\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:37:42.781057Z",
     "start_time": "2020-12-18T01:37:42.705234Z"
    }
   },
   "outputs": [],
   "source": [
    "f = codecs.open('d:/ds/project007/ds5/NER_all_cl_2.txt', encoding='utf8')\n",
    "train_ds = f.read()\n",
    "ori_text = re.findall('(?<=\\{\\\"originalText\\\": \\\")(.*?)(?=\\\", \\\"entities\\\":)', train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:37:45.033007Z",
     "start_time": "2020-12-18T01:37:45.026027Z"
    }
   },
   "outputs": [],
   "source": [
    "ori_text = [x.lower() for x in ori_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:54:00.044215Z",
     "start_time": "2020-12-11T08:53:59.936504Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in ori_text], density=True, cumulative=True, bins=[64*i for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:37:50.809570Z",
     "start_time": "2020-12-18T01:37:48.457487Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_str = [jieba.lcut(x[:512]) for x in ori_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T02:22:55.565264Z",
     "start_time": "2020-12-17T02:22:55.309469Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in cut_str], density=True, cumulative=True, bins=[64*i for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Normal Order Input Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Normal Order Input Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:37:56.004096Z",
     "start_time": "2020-12-18T01:37:56.000093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def marking(str_ls, return_tuple=True):\n",
    "    lbl_ls = [['I-'+str(i) if len(sub_str)>1 else 'O' for i, _ in enumerate(sub_str)] for sub_str in str_ls if sub_str is not None]\n",
    "    if return_tuple == True:\n",
    "        lbl_tuple = [[(char, lbl) for char, lbl in zip(sub_str, sub_lbl)] for sub_str, sub_lbl in zip(str_ls, lbl_ls)]\n",
    "        return lbl_tuple\n",
    "    else:\n",
    "        return lbl_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:37:58.026678Z",
     "start_time": "2020-12-18T01:37:57.376412Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lbl_str = [marking(x) for x in cut_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:00.398367Z",
     "start_time": "2020-12-18T01:38:00.344474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_ord_char_ls = [[x[0] for sublist in sent for x in sublist] for sent in lbl_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:04.252139Z",
     "start_time": "2020-12-18T01:38:04.210164Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_ord_labl_ls = [[x[1] for sublist in sent for x in sublist] for sent in lbl_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:07.375700Z",
     "start_time": "2020-12-18T01:38:07.370681Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all([len(char)==len(labl) for char, labl in zip(nrm_ord_char_ls, nrm_ord_labl_ls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:10.767642Z",
     "start_time": "2020-12-18T01:38:10.632993Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_ord_word_ls = copy.deepcopy(cut_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Normal Order Input Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:13.016662Z",
     "start_time": "2020-12-18T01:38:12.936848Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"D:/NLP/roberta-wwm-ext\",\n",
    "                                          return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:15.235737Z",
     "start_time": "2020-12-18T01:38:14.923533Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_ord_tokn_ls = [tokenizer.convert_tokens_to_ids(x) for x in nrm_ord_char_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:17.305164Z",
     "start_time": "2020-12-18T01:38:17.285217Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_labl_types = [list(set(sublist)) for sublist in nrm_ord_labl_ls]\n",
    "all_labl_types = list(set([item for sublist in all_labl_types for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:19.369643Z",
     "start_time": "2020-12-18T01:38:19.366651Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_labl_types.append('PAD')\n",
    "lbl2idx = {l: i for i, l in enumerate(all_labl_types)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:22.996941Z",
     "start_time": "2020-12-18T01:38:22.961037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_input_ids = pad_sequences(nrm_ord_tokn_ls,\n",
    "                              maxlen=512,\n",
    "                              dtype='long',\n",
    "                              value=0.0,\n",
    "                              truncating='post',\n",
    "                              padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:26.051817Z",
     "start_time": "2020-12-18T01:38:25.060423Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_attn_masks = [[float(i != 0.0) for i in ii] for ii in nrm_input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:29.072101Z",
     "start_time": "2020-12-18T01:38:29.040102Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_ord_word_set = deque(set([word for sublist in nrm_ord_word_ls for word in sublist]))\n",
    "nrm_ord_word_set.appendleft('_PAD_')\n",
    "nrm_ord_word_set = list(nrm_ord_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:31.542410Z",
     "start_time": "2020-12-18T01:38:31.536426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_word2idx = {w: i for i, w in enumerate(nrm_ord_word_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:34.650395Z",
     "start_time": "2020-12-18T01:38:34.593293Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_word_lbl = pad_sequences([[nrm_word2idx.get(w) for w in sublist] for sublist in nrm_ord_word_ls], maxlen=256, value=nrm_word2idx['_PAD_'], padding='post', truncating='post', dtype='long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:38.131927Z",
     "start_time": "2020-12-18T01:38:37.969235Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_word_msk = [[float(i != nrm_word2idx['_PAD_']) for i in ii] for ii in nrm_word_lbl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# INVERSE Order Input Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:41.420006Z",
     "start_time": "2020-12-18T01:38:41.401056Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class inverse:\n",
    "    def __init__(self, lbl_pair, ratio):\n",
    "        super(inverse, self).__init__()\n",
    "        self.lbl_pair_ = lbl_pair\n",
    "        self.lbl_pair = copy.deepcopy(lbl_pair)\n",
    "        self.ratio = ratio\n",
    "        eff_phrs_idx = [[\n",
    "            i for i, sublist in enumerate(pair) if (len(\n",
    "                re.findall('[^\\u4e00-\\u9fa5]+', ''.join(\n",
    "                    [x[0] for x in sublist]))) == 0) and (\n",
    "                        len(''.join([x[0] for x in sublist])) > 1)\n",
    "        ] for pair in lbl_pair]\n",
    "        self.rnd_idx = [\n",
    "            np.random.choice(x,\n",
    "                             size=math.floor(ratio * len(x)),\n",
    "                             replace=False).tolist() for x in eff_phrs_idx\n",
    "        ]\n",
    "\n",
    "    def inverse_pair(self):\n",
    "#         super(inverse, self).inverse_pair()\n",
    "        [[random.shuffle(pairs[i]) for i in idxs] for pairs, idxs in zip(self.lbl_pair, self.rnd_idx)]\n",
    "#         [random.shuffle(self.lbl_pair[i]) for i in self.rnd_idx]\n",
    "        self.inv_ord_char_ls = [[x[0] for sublist in sent for x in sublist]\n",
    "                                for sent in self.lbl_pair]\n",
    "        self.inv_ord_labl_ls = [[x[1] for sublist in sent for x in sublist]\n",
    "                                for sent in self.lbl_pair]\n",
    "        assert all([\n",
    "            len(char) == len(labl)\n",
    "            for char, labl in zip(self.inv_ord_char_ls, self.inv_ord_labl_ls)\n",
    "        ])\n",
    "        return self.inv_ord_char_ls, self.inv_ord_labl_ls\n",
    "\n",
    "    def shattered(self):\n",
    "#         super(inverse, self).shattered()\n",
    "        self.inv_ord_word_ls = [[\n",
    "            ''.join([x[0] for x in sublist]) for sublist in sent\n",
    "        ] for sent in self.lbl_pair]\n",
    "        return self.inv_ord_word_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T08:54:27.833027Z",
     "start_time": "2020-12-11T08:54:25.447279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv = inverse(lbl_str, ratio=0.5)\n",
    "inv_ord_char_ls, inv_ord_labl_ls = inv.inverse_pair()\n",
    "inv_ord_word_ls = inv.shattered()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## SHUF INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:38:46.788648Z",
     "start_time": "2020-12-18T01:38:46.784658Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "shuf_ratio_init = 0.6\n",
    "shuf_ratio_min = 0.1\n",
    "shuf_ratio = [\n",
    "    i for i in np.arange(shuf_ratio_init, shuf_ratio_min,\n",
    "                         -(shuf_ratio_init - shuf_ratio_min) / epochs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:16.130223Z",
     "start_time": "2020-12-18T01:38:51.529995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_ord_char_all = []\n",
    "inv_ord_labl_all = []\n",
    "inv_ord_word_all = []\n",
    "for i in range(epochs):\n",
    "    inv = inverse(lbl_str, ratio=shuf_ratio[i])\n",
    "    inv_ord_char_ls, inv_ord_labl_ls = inv.inverse_pair()\n",
    "    inv_ord_word_ls = inv.shattered()\n",
    "    inv_ord_char_all.append(inv_ord_char_ls)\n",
    "    inv_ord_labl_all.append(inv_ord_labl_ls)\n",
    "    inv_ord_word_all.append(inv_ord_word_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:24.886026Z",
     "start_time": "2020-12-18T01:39:24.554638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = [sent for sublist in inv_ord_word_all for sent in sublist]\n",
    "inv_ord_word_all_set = deque(set([word for sublist in temp for word in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:28.318672Z",
     "start_time": "2020-12-18T01:39:28.314683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_ord_word_all_set.appendleft('_PAD_')\n",
    "inv_ord_word_all_set = list(inv_ord_word_all_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:30.076968Z",
     "start_time": "2020-12-18T01:39:30.072980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(inv_ord_word_all_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Inverse Order Input Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:36.384392Z",
     "start_time": "2020-12-18T01:39:33.257494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_ord_tokn_all = [[tokenizer.convert_tokens_to_ids(x) for x in sublist] for sublist in inv_ord_char_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:38.809672Z",
     "start_time": "2020-12-18T01:39:38.513426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_input_ids_all = [pad_sequences(x,\n",
    "                              maxlen=512,\n",
    "                              dtype='long',\n",
    "                              value=0.0,\n",
    "                              truncating='post',\n",
    "                              padding='post') for x in inv_ord_tokn_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:51.490828Z",
     "start_time": "2020-12-18T01:39:41.545296Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_attn_masks_all = [[[float(i != 0.0) for i in ii] for ii in sublist] for sublist in inv_input_ids_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:55.394520Z",
     "start_time": "2020-12-18T01:39:54.720059Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_lbl_all = [\n",
    "    pad_sequences([[lbl2idx.get(l) for l in lab] for lab in sublist],\n",
    "                  maxlen=512,\n",
    "                  value=lbl2idx['PAD'],\n",
    "                  padding='post',\n",
    "                  truncating='post',\n",
    "                  dtype='long') for sublist in inv_ord_labl_all\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:39:57.488679Z",
     "start_time": "2020-12-18T01:39:57.481702Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_word2idx = {w: i for i, w in enumerate(inv_ord_word_all_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:00.502717Z",
     "start_time": "2020-12-18T01:39:59.944086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_word_lbl = [\n",
    "    pad_sequences([[inv_word2idx.get(w) for w in sent] for sent in sublist],\n",
    "                  maxlen=256,\n",
    "                  value=inv_word2idx['_PAD_'],\n",
    "                  padding='post',\n",
    "                  truncating='post',\n",
    "                  dtype='long') for sublist in inv_ord_word_all\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:04.819392Z",
     "start_time": "2020-12-18T01:40:03.159487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_word_msk = [[[float(i != inv_word2idx['_PAD_']) for i in ii] for ii in sublist] for sublist in inv_word_lbl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Constructing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:12.420740Z",
     "start_time": "2020-12-18T01:40:12.416787Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=768, **kwargs):\n",
    "        super(WordEmbedding, self).__init__(**kwargs)\n",
    "        self.lut = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "    def forward(self, word):\n",
    "        return self.lut(word) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:18.006914Z",
     "start_time": "2020-12-18T01:40:18.000929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WordPositionEncoding(nn.Module):\n",
    "    def __init__(self, dropout, emb_size=768, max_len=256, **kwargs):\n",
    "        super(WordPositionEncoding, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, emb_size)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, emb_size, 2) * -(math.log(10000.0) / emb_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        emb = emb + torch.autograd.Variable(self.pe[:, :emb.size(1)], requires_grad=False)\n",
    "        return self.dropout(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:20.373611Z",
     "start_time": "2020-12-18T01:40:20.369595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = key.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask.unsqueeze(2)==0, -1e9)\n",
    "        \n",
    "    p_attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "        \n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:22.592648Z",
     "start_time": "2020-12-18T01:40:22.584701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head, emb_size, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert emb_size % head == 0\n",
    "        \n",
    "        self.d_head = emb_size // head\n",
    "        self.head = head\n",
    "        \n",
    "        self.q_lin = nn.Linear(emb_size, emb_size)\n",
    "        self.k_lin = nn.Linear(emb_size, emb_size)\n",
    "        self.v_lin = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "        self.concat_lin = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        self.query = self.q_lin(query).view(batch_size, -1, self.head, self.d_head).transpose(1, 2)\n",
    "        self.key = self.k_lin(key).view(batch_size, -1, self.head, self.d_head).transpose(1, 2)\n",
    "        self.value = self.v_lin(value).view(batch_size, -1, self.head, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        self.x, self.attn = attention(self.query, self.key, self.value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "        x = self.x.transpose(1, 2).contiguous().view(batch_size, -1, self.head * self.d_head)\n",
    "        \n",
    "        return query + self.concat_lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:25.602198Z",
     "start_time": "2020-12-18T01:40:25.594219Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 nrm_vocab_size,\n",
    "                 inv_vocab_size,\n",
    "                 bert_model,\n",
    "                 cls,\n",
    "                 dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.nrm_wd_emb = WordEmbedding(vocab_size=nrm_vocab_size)\n",
    "        self.inv_wd_emb = WordEmbedding(vocab_size=inv_vocab_size)\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        self.attn1 = MultiHeadedAttention(head=12,\n",
    "                                          emb_size=768,\n",
    "                                          dropout=dropout)\n",
    "        self.attn2 = MultiHeadedAttention(head=12,\n",
    "                                          emb_size=768,\n",
    "                                          dropout=dropout)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(768, eps=1e-6)\n",
    "        self.lastdrop = nn.Dropout(p=dropout)\n",
    "        self.pooler = nn.Linear(in_features=768, out_features=cls)\n",
    "\n",
    "    def forward(self, _nrm_word_lbl, _nrm_word_msk, _nrm_input_ids,\n",
    "                _nrm_attn_masks, _inv_word_lbl, _inv_word_msk,\n",
    "                _inv_input_ids, _inv_attn_masks, _inv_lbls):\n",
    "        \n",
    "        q1 = self.nrm_wd_emb(_nrm_word_lbl)\n",
    "        k1 = self.bert_model(_inv_input_ids, attention_mask=_inv_attn_masks)[0]\n",
    "        v1 = self.bert_model(_nrm_input_ids, attention_mask=_nrm_attn_masks)[0]\n",
    "        attn1_res = self.layernorm(self.attn1(q1, k1, v1, mask=_nrm_word_msk))\n",
    "        q2 = k1\n",
    "        k2 = self.inv_wd_emb(_inv_word_lbl)\n",
    "        v2 = attn1_res\n",
    "        attn2_res = self.layernorm(self.attn2(q2, k2, v2, mask=_inv_attn_masks))\n",
    "        \n",
    "        output = self.pooler(self.lastdrop(attn2_res))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:34.056900Z",
     "start_time": "2020-12-18T01:40:29.408019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bert_mdl = BertModel.from_pretrained('D:/NLP/roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:37.953473Z",
     "start_time": "2020-12-18T01:40:37.890653Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_word_lbl = torch.LongTensor(nrm_word_lbl)\n",
    "nrm_word_msk = torch.LongTensor(nrm_word_msk)\n",
    "nrm_input_ids = torch.LongTensor(nrm_input_ids)\n",
    "nrm_attn_masks = torch.LongTensor(nrm_attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:41.146931Z",
     "start_time": "2020-12-18T01:40:40.617350Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inv_word_lbl = [torch.LongTensor(x) for x in inv_word_lbl]\n",
    "inv_word_msk = [torch.LongTensor(x) for x in inv_word_msk]\n",
    "inv_input_ids_all = [torch.LongTensor(x) for x in inv_input_ids_all]\n",
    "inv_attn_masks_all = [torch.LongTensor(x) for x in inv_attn_masks_all]\n",
    "inv_lbl_all = [torch.LongTensor(x) for x in inv_lbl_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:45.203117Z",
     "start_time": "2020-12-18T01:40:45.201093Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nrm_vocab_size = len(nrm_word2idx)\n",
    "inv_vocab_size = len(inv_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:51.044916Z",
     "start_time": "2020-12-18T01:40:48.880677Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ENC = Encoder(nrm_vocab_size=nrm_vocab_size,\n",
    "              inv_vocab_size=inv_vocab_size,\n",
    "              bert_model=bert_mdl,\n",
    "              cls=len(lbl2idx)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:40:59.116300Z",
     "start_time": "2020-12-18T01:40:59.113309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_steps = nrm_word_lbl.shape[0] * epochs\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:41:02.535184Z",
     "start_time": "2020-12-18T01:41:02.529173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ENC_optimizer = AdamW(ENC.parameters(), lr=1e-4)\n",
    "ENC_scheduler = get_linear_schedule_with_warmup(\n",
    "    ENC_optimizer,\n",
    "    num_warmup_steps=20,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:41:35.262520Z",
     "start_time": "2020-12-18T01:41:35.258534Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T01:41:39.610857Z",
     "start_time": "2020-12-18T01:41:39.120170Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "for i in range(epochs):\n",
    "    train_data = torch.utils.data.TensorDataset(\n",
    "        nrm_word_lbl, nrm_word_msk, nrm_input_ids, nrm_attn_masks,\n",
    "        inv_word_lbl[i], inv_word_msk[i], inv_input_ids_all[i],\n",
    "        inv_attn_masks_all[i], inv_lbl_all[i])\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                   sampler=train_sampler,\n",
    "                                                   batch_size=1)\n",
    "\n",
    "    ENC.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_nrm_word_lbl, b_nrm_word_msk, b_nrm_input_ids, b_nrm_attn_masks, b_inv_word_lbl, b_inv_word_msk, b_inv_input_ids, b_inv_attn_masks, b_inv_lbls = batch\n",
    "        \n",
    "        ENC.zero_grad()\n",
    "        \n",
    "        outputs = ENC(b_nrm_word_lbl, b_nrm_word_msk, b_nrm_input_ids, b_nrm_attn_masks, b_inv_word_lbl, b_inv_word_msk, b_inv_input_ids, b_inv_attn_masks, b_inv_lbls)\n",
    "        print(i, 'epoch', step, 'step', outputs.shape)\n",
    "        \n",
    "        loss = loss_fn(outputs.permute(0, 2, 1), b_inv_lbls)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=ENC.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        ENC_optimizer.step()\n",
    "        ENC_scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    loss_values.append(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T02:59:22.982124Z",
     "start_time": "2020-12-18T02:59:16.654880Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ENC.bert_model.save_pretrained('d:/NLP/00inv_ner/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNSTREAM TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.195911Z",
     "start_time": "2020-12-18T09:48:52.187905Z"
    }
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import seqeval.metrics\n",
    "import torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.339504Z",
     "start_time": "2020-12-18T09:48:52.196881Z"
    }
   },
   "outputs": [],
   "source": [
    "f = codecs.open('d:/ds/project007/ds5/NER_all_cl.txt', encoding='utf8')\n",
    "train_ds = f.read()\n",
    "\n",
    "ori_text = re.findall('(?<=\\{\\\"originalText\\\": \\\")(.*?)(?=\\\", \\\"entities\\\":)', train_ds)\n",
    "ents = re.findall('(?<=\\\"entities\\\": \\[)(.*)(?=\\]\\})', train_ds)\n",
    "lbl_names = [re.findall('(?<=\\\"label_type\\\": \")(.*?)(?=\\\")', x) for x in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.416423Z",
     "start_time": "2020-12-18T09:48:52.340497Z"
    }
   },
   "outputs": [],
   "source": [
    "start_pos = [re.findall('(?<=\\\"start_pos\\\": )([0-9]*)', x) for x in ents]\n",
    "end_pos = [re.findall('(?<=\\\"end_pos\\\": )([0-9]*)', x) for x in ents]\n",
    "\n",
    "start_pos = [np.array(x, dtype=np.int) for x in start_pos]\n",
    "end_pos = [np.array(x, dtype=np.int) for x in end_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.438239Z",
     "start_time": "2020-12-18T09:48:52.417291Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_names_flatten = [item for sublist in lbl_names for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.586957Z",
     "start_time": "2020-12-18T09:48:52.439232Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_enc = LabelEncoder()\n",
    "lbl_enc.fit(list(set(lbl_names_flatten)))\n",
    "lbl_enc.classes_\n",
    "lbl_codes = [lbl_enc.transform(x) for x in lbl_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.608784Z",
     "start_time": "2020-12-18T09:48:52.587835Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_codes = [x.astype('str_') for x in lbl_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.670612Z",
     "start_time": "2020-12-18T09:48:52.609776Z"
    }
   },
   "outputs": [],
   "source": [
    "lbl_marks = list(zip(start_pos, end_pos, lbl_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.751493Z",
     "start_time": "2020-12-18T09:48:52.672609Z"
    }
   },
   "outputs": [],
   "source": [
    "y_init = [np.array(['O']*len(x), dtype=object) for x in ori_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:52.812240Z",
     "start_time": "2020-12-18T09:48:52.752424Z"
    }
   },
   "outputs": [],
   "source": [
    "def cat_sub(marks, init_seq_):\n",
    "    st_pos = marks[0]\n",
    "    ed_pos = marks[1]\n",
    "    cats = marks[2]\n",
    "    assert len(st_pos) == len(ed_pos) == len(cats)\n",
    "    init_seq = copy.deepcopy(init_seq_)\n",
    "#     init_seq = init_seq.tolist()\n",
    "    for i in range(len(st_pos)):\n",
    "        init_seq[st_pos[i]:ed_pos[i]] = 'I-'+ cats[i]\n",
    "        init_seq[st_pos[i]] = 'B-' + cats[i]\n",
    "        \n",
    "    return init_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:53.147437Z",
     "start_time": "2020-12-18T09:48:52.813233Z"
    }
   },
   "outputs": [],
   "source": [
    "y = [cat_sub(lbl_marks[i], y_init[i]) for i in range(len(y_init))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:53.153322Z",
     "start_time": "2020-12-18T09:48:53.148377Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleantext(text_, lbl_seq_):\n",
    "#     _ = [list(range(i.start(), i.end())) for i in re.finditer('[^\\u4e00-\\u9fa5]+', text_)]\n",
    "    _ = [list(range(i.start(), i.end())) for i in re.finditer(r'\\\\', text_)]\n",
    "    non_chs_idx = [item for sublist in _ for item in sublist]\n",
    "    lbl_seq = copy.deepcopy(lbl_seq_)\n",
    "    lbl_seq = np.delete(lbl_seq, non_chs_idx)\n",
    "    \n",
    "    text = copy.deepcopy(text_)\n",
    "    text = re.sub(r'\\\\', '', text)\n",
    "    \n",
    "    assert len(text) == len(lbl_seq)\n",
    "    return text, lbl_seq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:53.350924Z",
     "start_time": "2020-12-18T09:48:53.154319Z"
    }
   },
   "outputs": [],
   "source": [
    "all_types = [np.unique(x).tolist() for x in y]\n",
    "all_types = [item for sublist in all_types for item in sublist]\n",
    "all_types = set(all_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:53.369743Z",
     "start_time": "2020-12-18T09:48:53.351792Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"D:/NLP/00inv_ner/K_INV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:48:53.454517Z",
     "start_time": "2020-12-18T09:48:53.370740Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:16.718651Z",
     "start_time": "2020-12-18T09:48:53.455514Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(ori_text, y)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:16.723600Z",
     "start_time": "2020-12-18T09:49:16.719608Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:16.796404Z",
     "start_time": "2020-12-18T09:49:16.724597Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_values = list(all_types)\n",
    "tag_values.append('PAD')\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:17.269139Z",
     "start_time": "2020-12-18T09:49:16.797402Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=512, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:17.340947Z",
     "start_time": "2020-12-18T09:49:17.270137Z"
    }
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=512, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:18.356236Z",
     "start_time": "2020-12-18T09:49:17.341945Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:18.367203Z",
     "start_time": "2020-12-18T09:49:18.359227Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_inputs = input_ids[:1000]\n",
    "val_inputs = input_ids[1000:]\n",
    "\n",
    "tr_tags = tags[:1000]\n",
    "val_tags = tags[1000:]\n",
    "\n",
    "tr_masks = attention_masks[:1000]\n",
    "val_masks = attention_masks[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:18.459985Z",
     "start_time": "2020-12-18T09:49:18.368200Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs, dtype=torch.long)\n",
    "val_inputs = torch.tensor(val_inputs, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tr_tags, dtype=torch.long)\n",
    "val_tags = torch.tensor(val_tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(tr_masks, dtype=torch.long)\n",
    "val_masks = torch.tensor(val_masks, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:18.497853Z",
     "start_time": "2020-12-18T09:49:18.463972Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=1)\n",
    "\n",
    "valid_data = torch.utils.data.TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = torch.utils.data.SequentialSampler(valid_data)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_data, sampler=valid_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:22.449357Z",
     "start_time": "2020-12-18T09:49:18.499847Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'D:/NLP/00inv_ner/K_INV/',\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:22.458295Z",
     "start_time": "2020-12-18T09:49:22.450316Z"
    }
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params':\n",
    "        [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate':\n",
    "        0.01\n",
    "    }, {\n",
    "        'params':\n",
    "        [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate':\n",
    "        0.0\n",
    "    }]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\n",
    "        \"params\": [p for n, p in param_optimizer]\n",
    "    }]\n",
    "\n",
    "bert_optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:22.541095Z",
     "start_time": "2020-12-18T09:49:22.459292Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:22.608001Z",
     "start_time": "2020-12-18T09:49:22.542072Z"
    }
   },
   "outputs": [],
   "source": [
    "crf_model = torchcrf.CRF(len(tag2idx), batch_first=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:49:22.690673Z",
     "start_time": "2020-12-18T09:49:22.608893Z"
    }
   },
   "outputs": [],
   "source": [
    "crf_optimizer = AdamW(crf_model.parameters(), lr=8e-5)\n",
    "crf_scheduler = get_linear_schedule_with_warmup(\n",
    "    crf_optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "bert_loss_values, loss_values, validation_loss_values = [], [], []\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    crf_model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    bert_total_loss = 0\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        crf_model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        bert_outputs = model(b_input_ids,\n",
    "                        token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        bert_loss = bert_outputs[0]\n",
    "        loss = crf_model(bert_outputs[1], b_labels, mask=b_input_mask.bool())\n",
    "        # get the loss\n",
    "        loss = -loss\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        bert_total_loss += bert_loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(),\n",
    "                                       max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        bert_optimizer.step()\n",
    "        crf_optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "#         print('now processing ', step, ' step')\n",
    "        crf_scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    avg_bert_loss = bert_total_loss / len(train_dataloader)\n",
    "    \n",
    "    print('Average Bert loss: {}'.format(avg_bert_loss))\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    bert_loss_values.append(avg_bert_loss)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    crf_paths = []\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    crf_model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            bert_outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "            loss_ = crf_model(bert_outputs[0], b_labels, mask=b_input_mask.bool())\n",
    "            # Move logits and labels to CPU\n",
    "        bert_logits = bert_outputs[0].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        crf_path = crf_model.decode(emissions=bert_outputs[0], mask=b_input_mask.bool())\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "#         eval_loss += -loss_\n",
    "        crf_paths.extend(crf_path)\n",
    "        predictions.extend([list(p) for p in np.argmax(bert_logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    crf_tags = [[\n",
    "        tag_values[c_i] for c_i, l_i in zip(c, l) if tag_values[l_i] != 'PAD'\n",
    "    ] for c, l in zip(crf_paths, true_labels)]\n",
    "\n",
    "    pred_tags = [[\n",
    "        tag_values[p_i] for p_i, l_i in zip(p, l) if tag_values[l_i] != 'PAD'\n",
    "    ] for p, l in zip(predictions, true_labels)]\n",
    "\n",
    "    valid_tags = [[tag_values[l_i] for l_i in l if tag_values[l_i] != 'PAD']\n",
    "                  for l in true_labels]\n",
    "\n",
    "    print(seqeval.metrics.classification_report(valid_tags, pred_tags, digits=3))\n",
    "    print('\\n')\n",
    "    print(seqeval.metrics.classification_report(valid_tags, crf_tags, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:27:16.479451Z",
     "start_time": "2020-12-18T09:27:16.381444Z"
    }
   },
   "outputs": [],
   "source": [
    "crf_tags = [[\n",
    "    tag_values[c_i] for c_i, l_i in zip(c, l) if tag_values[l_i] != 'PAD'\n",
    "] for c, l in zip(crf_paths, true_labels)]\n",
    "\n",
    "pred_tags = [[\n",
    "    tag_values[p_i] for p_i, l_i in zip(p, l) if tag_values[l_i] != 'PAD'\n",
    "] for p, l in zip(predictions, true_labels)]\n",
    "\n",
    "valid_tags = [[tag_values[l_i] for l_i in l if tag_values[l_i] != 'PAD']\n",
    "              for l in true_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T09:28:45.385502Z",
     "start_time": "2020-12-18T09:28:42.464207Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(seqeval.metrics.classification_report(valid_tags, pred_tags, digits=3))\n",
    "print('\\n')\n",
    "print(seqeval.metrics.classification_report(valid_tags, crf_tags, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
